{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PULL directly from Kaggle with API Key\n"
      ],
      "metadata": {
        "id": "JNvvM-e1jmWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kaggle\n",
        "\n",
        "def setup_kaggle_credentials():\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "    # Path to the kaggle.json\n",
        "    # Download API KEY JSON from Kaggle\n",
        "    kaggle_api_path = Path.home() / '.kaggle' / 'kaggle.json'\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = str(kaggle_api_path.parent)\n",
        "\n",
        "    if os.name != 'nt':  # Not Windows\n",
        "        kaggle_api_path.chmod(0o600)\n",
        "\n",
        "    api = KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "def download_dataset(dataset_path):\n",
        "    kaggle.api.dataset_download_files(dataset_path, path='./', unzip=True)\n",
        "\n",
        "# Set up credentials\n",
        "setup_kaggle_credentials()\n",
        "\n",
        "# Download dataset\n",
        "dataset_path = 'dataset-owner/dataset-name'\n",
        "download_dataset(dataset_path)\n"
      ],
      "metadata": {
        "id": "1PmM46HWjnLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract ZIP Files"
      ],
      "metadata": {
        "id": "rqsDlEMCrtJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "with zipfile.ZipFile(\"/content/UCI_Credit_Card.csv.zip\",\"r\") as z:\n",
        "    for filename in z.namelist():\n",
        "        if filename.endswith(\".csv\"):\n",
        "            with z.open(filename) as path:\n",
        "                data = pd.read_csv(path)\n",
        "\n"
      ],
      "metadata": {
        "id": "vsBi9Q04rsmQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoML Classification"
      ],
      "metadata": {
        "id": "WbceM6nBXYw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering for Classification\n",
        "\n",
        "\n",
        "*   Pearson Correlation among the features\n",
        "*   Mutual Info Gain of the features from above with target\n",
        "\n"
      ],
      "metadata": {
        "id": "o5bc-76QYsgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# CELL\n",
        "# corr_series = data.corr().loc['default.payment.next.month',:][:-1].sort_values(key=lambda x: abs(x),ascending = False,)\n",
        "target_column = \"default.payment.next.month\"\n",
        "df_corr = data.drop(columns = [target_column,\"ID\"]).corr()\n",
        "correlation_drop_threshold = 0.9\n",
        "df_corr = df_corr.where(np.triu(np.ones(df_corr.shape), k = 0).astype(bool))\n",
        "df_corr = df_corr.reset_index().melt(id_vars = \"index\", var_name = \"feature\", value_name  = \"correlation_score\")\n",
        "df_corr = df_corr[(df_corr[\"correlation_score\"]!= 1)&\n",
        "        (df_corr[\"correlation_score\"]>=0.8)]\n",
        "df_corr\n",
        "#Logic to find similar features and drop them\n",
        "chain_lst = []\n",
        "\n",
        "for feature_1, feature_2 in zip(df_corr['index'], df_corr['feature']):\n",
        "    if len(chain_lst)==0:\n",
        "        lst = [feature_1,feature_2]\n",
        "        chain_lst.append(lst)\n",
        "    else:\n",
        "        #Check for feature\n",
        "        check = 0\n",
        "        for i,lst in enumerate(chain_lst):\n",
        "            if (feature_1 in lst) or (feature_2 in lst):\n",
        "                lst = [feature_1,feature_2]\n",
        "                chain_lst[i] = chain_lst[i] + lst\n",
        "                chain_lst[i] = list(set(chain_lst[i]))\n",
        "                check = check + 1\n",
        "                break\n",
        "        if check == 0:\n",
        "            lst = [feature_1,feature_2]\n",
        "            chain_lst.append(lst)\n",
        "\n",
        "feature_drop_lst =[\"ID\"]\n",
        "for lst in chain_lst:\n",
        "  feature_drop_lst = feature_drop_lst + lst[1:]\n",
        "data = data.drop(columns = feature_drop_lst)\n",
        "\n",
        "X = data.loc[:,data.columns != target_column]\n",
        "y = data[target_column]\n",
        "\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "importances = mutual_info_classif(X,y.astype(int))\n",
        "feat_importances = pd.Series(importances,X.columns)\n",
        "\n",
        "result['Information_gain'] = feat_importances\n",
        "\n",
        "info_gain_threshold = 0.005\n",
        "drop_feature_lst = result[result[\"Information_gain\"]<info_gain_threshold].index\n",
        "data = data.drop(columns = drop_feature_lst)"
      ],
      "metadata": {
        "id": "rqEGXfIduSrp"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Upload to S3"
      ],
      "metadata": {
        "id": "5naLTLMYXo8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "X = data.drop(columns = [target_column])\n",
        "\n",
        "\n",
        "# CELL\n",
        "from sklearn.model_selection import train_test_split\n",
        "y = data[target_column]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y.astype(int), test_size=0.3, random_state=42)\n",
        "\n",
        "try:\n",
        "    from flaml import AutoML\n",
        "except:\n",
        "    !pip install flaml\n",
        "    !pip install catboost\n",
        "    from flaml import AutoML\n",
        "\n",
        "automl = AutoML()\n",
        "models = ['rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1']\n",
        "# settings = {\n",
        "#     \"time_budget\": time_budget,  # total running time in seconds\n",
        "#     \"metric\": metric,\n",
        "#     \"estimator_list\": ['lgbm','rf', 'catboost', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'lrl1'],\n",
        "#                         # check the documentation for options of metrics (https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML#optimization-metric)\n",
        "#     \"task\": 'classification',  # task type\n",
        "#     \"log_file_name\": 'airlines_experiment.log',  # flaml log file\n",
        "#     \"seed\": 42,    # random seed\n",
        "# }\n",
        "\n",
        "\n",
        "# automl.fit(X_train=X_train, y_train=y_train, metric = score,task = 'classification',estimator_list = models.split(','),time_budget = time,verbose = 0,seed = 42)\n",
        "automl.fit(X_train=X_train, y_train=y_train, metric = \"f1\",task = 'classification',estimator_list = models,time_budget = 300,verbose = 0)\n",
        "\n",
        "result1 = pd.DataFrame(1 - pd.Series(automl.best_loss_per_estimator),columns = ['metric']).reset_index(inplace = False).rename(columns = {'index':'models'})\n",
        "result_target = pd.DataFrame()\n",
        "result_target['Id'] = [val for val in range(len(y_test))]\n",
        "result_target[target_column] = y_test.to_list()\n",
        "result_target[f'Predicted_{target_column}'] = automl.predict(X_test)\n",
        "result = pd.concat([result1,result_target],axis = 1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHj53sFVT30Z",
        "outputId": "2db8d4c5-b883-4e6a-f2d6-2856dbad6d70"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:flaml.tune.searcher.blendsearch:No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'. More info can be found at https://microsoft.github.io/FLAML/docs/FAQ#about-low_cost_partial_config-in-tune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL\n",
        "import os\n",
        "import pickle\n",
        "import hashlib\n",
        "\n",
        "#!pip install boto3\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# CELL\n",
        " BUCKET_PATH = \"Enter Bucket Name Here\"\n",
        "s3_dir_name = \"Enter Directory Name Here\"\n",
        "client_name = \"Enter Folder Name Here\"\n",
        "STAGING_PATH = \"/tmp/staging/common/\"\n",
        "\n",
        "# CELL\n",
        "def dump_object(python_object, path):\n",
        "    # Obtain directory path from input path\n",
        "    directory_path = os.path.dirname(path)\n",
        "    # If directory does not exist, create it\n",
        "    if not os.path.exists(directory_path):\n",
        "        os.makedirs(directory_path)\n",
        "    # Dump\n",
        "    with open(path, \"wb\") as output_file:\n",
        "        pickle.dump(python_object, output_file)\n",
        "\n",
        "# CELL\n",
        "def upload(bucket,path,s3_path,metadata={},config=None,):\n",
        "    # Create a client\n",
        "\n",
        "    client = boto3.client(\"s3\")\n",
        "    # Upload a file\n",
        "    client.upload_file(\n",
        "        path, bucket, s3_path, ExtraArgs={\"Metadata\": metadata}, Config=config\n",
        "    )\n",
        "\n",
        "# CELL\n",
        "def delete_file(path):\n",
        "    try:\n",
        "        os.remove(path)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "# CELL\n",
        "def upload_object(\n",
        "    python_object,\n",
        "    bucket,\n",
        "    s3_path,\n",
        "    metadata={},\n",
        "    config=None,\n",
        "):\n",
        "    # Define a temporary path\n",
        "    temporary_path = os.path.join(\n",
        "        STAGING_PATH,\n",
        "        generate_temporary_filename(path=s3_path, extension=\"pickle\"),\n",
        "    )\n",
        "    # Dump Python object to temporary pickle file\n",
        "    dump_object(python_object=python_object, path=temporary_path)\n",
        "    # Upload pickle file\n",
        "    upload(\n",
        "        bucket=bucket,\n",
        "        path=temporary_path,\n",
        "        s3_path=s3_path,\n",
        "        metadata=metadata,\n",
        "        config=config,\n",
        "    )\n",
        "    # Clean up temporary pickle file\n",
        "    delete_file(path=temporary_path)\n",
        "\n",
        "# CELL\n",
        "def generate_temporary_filename(path, extension):\n",
        "    filename = hashlib.sha256(path.encode(\"utf-8\")).hexdigest()\n",
        "    return \"%s.%s\" % (filename, extension)\n",
        "\n",
        "# CELL\n",
        "def generate_temporary_directory(path):\n",
        "    directory = hashlib.sha256(path.encode(\"utf-8\")).hexdigest()\n",
        "    return directory\n",
        "\n",
        "# CELL\n",
        "s3_path = f\"{s3_dir_name}/{client_name}/automl.pkl\"\n",
        "\n",
        "\n",
        "# CELL\n",
        "upload_object(\n",
        "    python_object=automl.model,\n",
        "    bucket=BUCKET_PATH,\n",
        "    s3_path=s3_path,\n",
        "    metadata={},\n",
        "    config=None,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "2cApioNNXh3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pull the model from Amazon S3 and Predict"
      ],
      "metadata": {
        "id": "z2IX4k-5Xx_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import hashlib\n",
        "#!pip install boto3\n",
        "import boto3\n",
        "import botocore\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# CELL\n",
        " BUCKET_PATH = \"Enter Bucket Name Here\"\n",
        "s3_dir_name = \"Enter Directory Name Here\"\n",
        "client_name = \"Enter Folder Name Here\"\n",
        "STAGING_PATH = \"/tmp/staging/common/\"\n",
        "\n",
        "def download(\n",
        "    bucket,\n",
        "    path,\n",
        "    s3_path,\n",
        "    config=None,\n",
        "\n",
        "):\n",
        "    # Create a client\n",
        "\n",
        "    client = boto3.client(\"s3\")\n",
        "\n",
        "    # Download a file\n",
        "    try:\n",
        "        # Obtain directory path from input path\n",
        "        directory_path = os.path.dirname(path)\n",
        "        # If directory does not exist, create it\n",
        "        if not os.path.exists(directory_path):\n",
        "            os.makedirs(directory_path)\n",
        "        # Download\n",
        "        client.download_file(bucket, s3_path, path, Config=config)\n",
        "    except botocore.exceptions.ClientError:\n",
        "        raise NoItemFoundException()\n",
        "\n",
        "\n",
        "def download_object(\n",
        "    bucket,\n",
        "    s3_path,\n",
        "    config=None,\n",
        "):\n",
        "    # Define a temporary path\n",
        "    temporary_path = os.path.join(\n",
        "        STAGING_PATH,\n",
        "        generate_temporary_filename(path=s3_path, extension=\"pickle\"),\n",
        "    )\n",
        "    # Download pickle file into temporary location\n",
        "    download(\n",
        "        bucket=bucket,\n",
        "        path=temporary_path,\n",
        "        s3_path=s3_path,\n",
        "        config=config,\n",
        "    )\n",
        "    # Load pickle file as Python object\n",
        "    python_object = load_object(path=temporary_path)\n",
        "    # Clean up temporary pickle file\n",
        "    delete_file(path=temporary_path)\n",
        "\n",
        "    return python_object\n",
        "\n",
        "def delete_file(path):\n",
        "    try:\n",
        "        os.remove(path)\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "def load_object(path):\n",
        "    with open(path, \"rb\") as input_file:\n",
        "        return pickle.load(input_file)\n",
        "\n",
        "def generate_temporary_filename(path, extension):\n",
        "    filename = hashlib.sha256(path.encode(\"utf-8\")).hexdigest()\n",
        "    return \"%s.%s\" % (filename, extension)\n",
        "\n",
        "\n",
        "def generate_temporary_directory(path):\n",
        "    directory = hashlib.sha256(path.encode(\"utf-8\")).hexdigest()\n",
        "    return directory\n",
        "\n",
        "\n",
        "s3_path = f\"{s3_dir_name}/{client_name}/automl.pkl\"\n",
        "\n",
        "# CELL\n",
        "automl = download_object(\n",
        "    BUCKET_PATH,\n",
        "    s3_path)\n",
        "from flaml import AutoML\n",
        "\n",
        "\n",
        "X = data['input']\n",
        "# X = X.drop(columns = [target_column])\n",
        "\n",
        "\n",
        "\n",
        "result = pd.DataFrame()\n",
        "\n",
        "result['Id'] = [val for val in range(1,len(X)+1)]\n",
        "\n",
        "result[f'Predicted_{target_column}'] = automl.predict(X)\n",
        "\n"
      ],
      "metadata": {
        "id": "0teJtBFBXxjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "GaDYMtTVXOMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"model_1\"\n",
        "# overwrite = \"False\"\n",
        "# model_description = \"\"\n",
        "# target_column = \"math score\"\n",
        "# feature_columns = \"gender,race/ethnicity,parental level of education,lunch,test preparation course\"#.split(\",\")\n",
        "# model_list = \"lgbm,rf,catboost,xgboost,extra_tree,xgb_limitdepth\"#.split(\",\")\n",
        "# # model_list = \"lgbm,rf,xgboost,extra_tree,xgb_limitdepth\"#.split(\",\")\n",
        "\n",
        "\n",
        "# time_budget = 100\n",
        "# test_size = 0.25\n",
        "# r2_rmse_mse_mae = \"r2\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if (overwrite).lower()==\"true\":\n",
        "    # !pip install catboost\n",
        "    # !pip install \"flaml[automl]\"\n",
        "\n",
        "    import pandas as pd\n",
        "    import catboost\n",
        "\n",
        "    from flaml import AutoML\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    # data = pd.read_csv(path, low_memory = False)\n",
        "\n",
        "    automl = AutoML()\n",
        "\n",
        "    automl_settings = {\n",
        "        \"time_budget\": time_budget,  # in seconds\n",
        "        \"metric\": metric ,\n",
        "        \"estimator_list\":model_list.split(\",\"),\n",
        "        \"task\": 'regression',\n",
        "        \"log_file_name\": \"flaml.log\",\n",
        "    }\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data[feature_columns.split(\",\")], data[target_column], test_size = test_size)\n",
        "    automl.fit(X_train=X_train, y_train=y_train,\n",
        "           **automl_settings, verbose = 0)\n",
        "\n",
        "    model_log = pd.DataFrame()\n",
        "    model_log[\"train_date\"] = [str(datetime.now().date())]\n",
        "    model_log[\"train_time\"] = [str(datetime.now().time())]\n",
        "    model_log[\"model_name\"] = [model_name]\n",
        "    model_log[\"model_description\"] = [model_description]\n",
        "    model_log[\"best_model\"] = [automl.best_estimator]\n",
        "    if metric == \"r2\":\n",
        "      model_log[\"model_metrics\"] = [f\"{metric} : {round(1 - automl.best_loss,2)}\"]\n",
        "      model_log[\"model_hyperparameters\"] = [automl.best_config]\n",
        "      model_log[\"estimator_list\"] = [model_list]\n",
        "\n",
        "      model_log[\"best_loss_per_estimator\"] = [(1 - pd.Series(automl.best_loss_per_estimator)).to_dict()]\n",
        "    else:\n",
        "      model_log[\"model_metrics\"] = [f\"{metric} : {round(automl.best_loss,2)}\"]\n",
        "      model_log[\"model_hyperparameters\"] = [automl.best_config]\n",
        "      model_log[\"estimator_list\"] = [model_list]\n",
        "\n",
        "      model_log[\"best_loss_per_estimator\"] = [pd.Series(automl.best_loss_per_estimator).to_dict()]\n",
        "    #model_log[\"model_hyperparameters\"] = [automl.best_config]\n",
        "    #model_log[\"estimator_list\"] = [model_list]\n",
        "\n",
        "    model_log[\"time_budget\"] = [time_budget]\n",
        "    model_log[\"test_size\"] = [test_size]\n",
        "\n",
        "\n",
        "    df_feature_imp = pd.DataFrame()\n",
        "    df_feature_imp['Features'] = list(automl.feature_names_in_)\n",
        "    df_feature_imp['Importance Score'] = list(automl.feature_importances_)\n",
        "    df_feature_imp = df_feature_imp.sort_values(by = ['Importance Score'], ascending = False)\n",
        "\n",
        "    df_pred = pd.DataFrame()\n",
        "    df_pred = data.copy()\n",
        "    df_pred[f\"pred_{target_column}\"] = automl.predict(data[feature_columns.split(\",\")])\n",
        "    result = {\"log\":model_log,\n",
        "      \"feature_importance\":df_feature_imp,\n",
        "      \"preds\":df_pred\n",
        "    }\n",
        "\n",
        "    import os\n",
        "    import pickle\n",
        "    import hashlib\n",
        "\n",
        "    #!pip install boto3\n",
        "    import boto3\n",
        "    import io\n",
        "\n",
        "\n",
        "    # CELL\n",
        "    BUCKET_PATH = \"Enter Bucket Name Here\"\n",
        "    s3_dir_name = \"Enter Directory Name Here\"\n",
        "    client_name = \"Enter Folder Name Here\"\n",
        "    STAGING_PATH = \"/tmp/staging/common/\"\n",
        "\n",
        "    # CELL\n",
        "    def dump_object(python_object, path):\n",
        "        # Obtain directory path from input path\n",
        "        directory_path = os.path.dirname(path)\n",
        "        # If directory does not exist, create it\n",
        "        if not os.path.exists(directory_path):\n",
        "            os.makedirs(directory_path)\n",
        "        # Dump\n",
        "        with open(path, \"wb\") as output_file:\n",
        "            pickle.dump(python_object, output_file)\n",
        "\n",
        "    # CELL\n",
        "    def upload(bucket,path,s3_path,metadata={},config=None,):\n",
        "        # Create a client\n",
        "\n",
        "        client = boto3.client(\"s3\")\n",
        "        # Upload a file\n",
        "        client.upload_file(\n",
        "            path, bucket, s3_path, ExtraArgs={\"Metadata\": metadata}, Config=config\n",
        "        )\n",
        "\n",
        "    # CELL\n",
        "    def delete_file(path):\n",
        "        try:\n",
        "            os.remove(path)\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "    # CELL\n",
        "    def upload_object(\n",
        "        python_object,\n",
        "        bucket,\n",
        "        s3_path,\n",
        "        metadata={},\n",
        "        config=None,\n",
        "    ):\n",
        "        # Define a temporary path\n",
        "        temporary_path = os.path.join(\n",
        "            STAGING_PATH,\n",
        "            generate_temporary_filename(path=s3_path, extension=\"pickle\"),\n",
        "        )\n",
        "        # Dump Python object to temporary pickle file\n",
        "        dump_object(python_object=python_object, path=temporary_path)\n",
        "        # Upload pickle file\n",
        "        upload(\n",
        "            bucket=bucket,\n",
        "            path=temporary_path,\n",
        "            s3_path=s3_path,\n",
        "            metadata=metadata,\n",
        "            config=config,\n",
        "        )\n",
        "        # Clean up temporary pickle file\n",
        "        delete_file(path=temporary_path)\n",
        "\n",
        "    # CELL\n",
        "    def generate_temporary_filename(path, extension):\n",
        "        filename = hashlib.sha256(path.encode(\"utf-8\")).hexdigest()\n",
        "        return \"%s.%s\" % (filename, extension)\n",
        "\n",
        "    # CELL\n",
        "    def generate_temporary_directory(path):\n",
        "        directory = hashlib.sha256(path.encode(\"utf-8\")).hexdigest()\n",
        "        return directory\n",
        "\n",
        "    # CELL\n",
        "    s3_path = f\"{s3_dir_name}/{client_name}/{model_name}.pkl\"\n",
        "\n",
        "\n",
        "    # CELL\n",
        "    upload_object(\n",
        "        python_object=automl,\n",
        "        bucket=BUCKET_PATH,\n",
        "        s3_path=s3_path,\n",
        "        metadata={},\n",
        "        config=None,\n",
        "    )\n",
        "\n",
        "else:\n",
        "    import os\n",
        "    import pickle\n",
        "    import hashlib\n",
        "    #!pip install boto3\n",
        "    import boto3\n",
        "    import botocore\n",
        "    import pandas as pd\n",
        "    import io\n",
        "    import numpy as np\n",
        "    import random\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    # CELL\n",
        "    BUCKET_PATH = \"Enter Bucket Name Here\"\n",
        "    s3_dir_name = \"Enter Directory Name Here\"\n",
        "    client_name = \"Enter Folder Name Here\"\n",
        "    STAGING_PATH = \"/tmp/staging/common/\"\n",
        "\n",
        "    def download(\n",
        "        bucket,\n",
        "        path,\n",
        "        s3_path,\n",
        "        config=None,\n",
        "\n",
        "    ):\n",
        "        # Create a client\n",
        "\n",
        "        client = boto3.client(\"s3\")\n",
        "\n",
        "        # Download a file\n",
        "        try:\n",
        "            # Obtain directory path from input path\n",
        "            directory_path = os.path.dirname(path)\n",
        "            # If directory does not exist, create it\n",
        "            if not os.path.exists(directory_path):\n",
        "                os.makedirs(directory_path)\n",
        "            # Download\n",
        "            client.download_file(bucket, s3_path, path, Config=config)\n",
        "        except botocore.exceptions.ClientError:\n",
        "            raise NoItemFoundException()\n",
        "\n",
        "\n",
        "    def download_object(\n",
        "        bucket,\n",
        "        s3_path,\n",
        "        config=None,\n",
        "    ):\n",
        "        # Define a temporary path\n",
        "        temporary_path = os.path.join(\n",
        "            STAGING_PATH,\n",
        "            generate_temporary_filename(path=s3_path, extension=\"pickle\"),\n",
        "        )\n",
        "        # Download pickle file into temporary location\n",
        "        download(\n",
        "            bucket=bucket,\n",
        "            path=temporary_path,\n",
        "            s3_path=s3_path,\n",
        "            config=config,\n",
        "        )\n",
        "        # Load pickle file as Python object\n",
        "        python_object = load_object(path=temporary_path)\n",
        "        # Clean up temporary pickle file\n",
        "        delete_file(path=temporary_path)\n",
        "\n",
        "        return python_object\n",
        "\n",
        "    def delete_file(path):\n",
        "        try:\n",
        "            os.remove(path)\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "    def load_object(path):\n",
        "        with open(path, \"rb\") as input_file:\n",
        "            return pickle.load(input_file)\n",
        "\n",
        "    def generate_temporary_filename(path, extension):\n",
        "        filename = hashlib.sha256(path.encode(\"utf-8\")).hexdigest()\n",
        "        return \"%s.%s\" % (filename, extension)\n",
        "\n",
        "\n",
        "    def generate_temporary_directory(path):\n",
        "        directory = hashlib.sha256(path.encode(\"utf-8\")).hexdigest()\n",
        "        return directory\n",
        "\n",
        "\n",
        "    s3_path = f\"{s3_dir_name}/{client_name}/{model_name}.pkl\"\n",
        "\n",
        "    # CELL\n",
        "    automl = download_object(\n",
        "        BUCKET_PATH,\n",
        "        s3_path)\n",
        "    from flaml import AutoML\n",
        "\n",
        "\n",
        "    X = data[feature_columns.split(',')]\n",
        "\n",
        "\n",
        "\n",
        "    result = data.copy()\n",
        "\n",
        "    result[f\"pred_{target_column}\"] = automl.predict(X)"
      ],
      "metadata": {
        "id": "4tOd8jxyuGWR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}